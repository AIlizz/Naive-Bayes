# Naive-Bayes

## 简介
* 首先我们介绍几种概率的定义：<br>
条件概率P(A|B),在B的条件下，发生A的概率；<br>
先验概率P(B)，B发生的概率，P(A)，A发生的概率；<br>
后验概率P(B|A),表示A发生的条件下，时间B发生的概率；<br>
* 然后我们利用上面的几种概率，介绍几种新的概率：<br>
全概率公式：P(A) = P(A|B).P(B) = P(A|B1).P(B1) + P(A|B2).P(B2)，全概率公式表达的是事件A发生的概率，等于所有的条件概率之和。<br>
基于全概率公式我们可以对后验概率改写为条件概率和全概率公式表达：<br>
P(B|A) = P(B,A)/P(A) = P(A|B).P(B)/sum(P(A|Bi).P(Bi))<br>
上式就是贝叶斯(Bayes)公式
* 那么什么朴素贝叶斯公式呢？
对于后验概率P(B|A) = P(B,A)/P(A) = P(A|B).P(B)/P(A)来说，假设A中的很多特征是相互独立的，比如有（A1，A2，A3，A4），P(A|B) = P(A1|B).P(A2|B).P(A3|B).P(A4|B)
那么P(B|A) = P(B,A)/P(A) = P(A|B).P(B)/P(A) = P(A1|B).P(A2|B).P(A3|B).P(A4|B).P(B)/P(A)
这样就用条件概率（比如某些词在某类文章中的概率），先验概率（文章的概率）计算出后验概率（已知这次词，然后求是什么文章的概率）。

### 优点
* 坚实的数学基础，稳定的分类效率，所需估计的参数很少（超参数少）；
* 对缺失数据不太敏感，算法也比较简单

### 用法
文本分类，垃圾邮件的分类，信用评估，钓鱼网站监测

## 利用NB进行鸢尾花的分类
* 对于连续性数据的学习：鸢尾花的特征和标签都是连续性的，我们假设这些连续性的数据服从高斯分布（正态分布，用平均值，方差来评估），相应的模型需要用GaussianNB来评估。<br>

## 利用NB进行离散数据的分类
* 对于离散性数据的学习：学习资料给到的是用CategoricalNB进行分类
* 这里解释几行代码，主要是数据生成：<br>
rng = np.random.RandomState(1) # 给出随机种子，seed = 1<br>
X = rng.randint(5, size=(600, 100)) # 随机生成600个100维的数据，每一维的特征都是[0, 4]之前的整数<br>
y = np.array([1, 2, 3, 4, 5, 6] * 100) # 随机生成600 X 1维的数据，数据按照1, 2, 3, 4, 5, 6进行循环<br>
data = np.c_[X, y] # 将数据X,y进行左右拼接<br>
* 解释离散性特征（待更新）

## 总结（待更新）
