# Naive-Bayes

## 简介
* 首先我们介绍几种概率的定义：<br>
条件概率P(A|B),在B的条件下，发生A的概率；<br>
先验概率P(B)，B发生的概率，P(A)，A发生的概率；<br>
后验概率P(B|A),表示A发生的条件下，时间B发生的概率；<br>
* 然后我们利用上面的几种概率，介绍几种新的概率：<br>
全概率公式：P(A) = P(A|B).P(B) = P(A|B1).P(B1) + P(A|B2).P(B2)，全概率公式表达的是事件A发生的概率，等于所有的条件概率之和。<br>
基于全概率公式我们可以对后验概率改写为条件概率和全概率公式表达：<br>
P(B|A) = P(B,A)/P(A) = P(A|B).P(B)/sum(P(A|Bi).P(Bi))<br>
上式就是贝叶斯(Bayes)公式
* 那么什么朴素贝叶斯公式呢？
对于后验概率P(B|A) = P(B,A)/P(A) = P(A|B).P(B)/P(A)来说，假设A中的很多特征是相互独立的，比如有（A1，A2，A3，A4），P(A|B) = P(A1|B).P(A2|B).P(A3|B).P(A4|B)
那么P(B|A) = P(B,A)/P(A) = P(A|B).P(B)/P(A) = P(A1|B).P(A2|B).P(A3|B).P(A4|B).P(B)/P(A)
这样就用条件概率（比如某些词在某类文章中的概率），先验概率（文章的概率）计算出后验概率（已知这次词，然后求是什么文章的概率）。

### 优点
* 坚实的数学基础，稳定的分类效率，所需估计的参数很少（超参数少）；
* 对缺失数据不太敏感，算法也比较简单

### 用法
文本分类，垃圾邮件的分类，信用评估，钓鱼网站监测

## 利用NB进行鸢尾花的分类
鸢尾花的特征和标签都是




## 利用NB进行离散数据的分类
